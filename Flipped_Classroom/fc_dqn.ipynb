{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYe1z1Z_rGFT",
        "outputId": "5384af21-4ec6-4e5d-de42-c2c434e3e48a"
      },
      "outputs": [],
      "source": [
        "#conda install \"gymnasium[accept-rom-license, atari]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mUnrdc7lneyJ"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RRZmFxRexD1Y"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n-moving targets (optimizing to old targets)\\n-overestimation bias\\n\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Task 0: Recap DQN\n",
        "\n",
        "#0.1 Provide Pseudocode for DQN\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "target = r + gamma * maxa' [Q(s',a')]   #problem in terminal state t = if terminated: reward\n",
        "DQN ← \n",
        "ERP ← {}\n",
        "    t ← [s,a,r,s']\n",
        "    ERP += t\n",
        "        batch[s,a,r,s'] from ERP\n",
        "        DQN: regress\n",
        "            min MSE [Qdqn (s,a) - target]\n",
        "    DQNtarget ← delayedDQN\n",
        "\"\"\"\n",
        "\n",
        "#0.2 What are the two main issues DQN faces?\n",
        "\n",
        "\"\"\"\n",
        "-moving targets (optimizing to old targets)\n",
        "-overestimation bias\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8evTQ-ATuxYe"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'AtariEnv' object has no attribute 'actionspace'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\lena-\\Downloads\\fc_dqn.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lena-/Downloads/fc_dqn.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         observation, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lena-/Downloads/fc_dqn.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m#1.4 Which actions will be accepted?\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lena-/Downloads/fc_dqn.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAction Space\u001b[39m\u001b[39m\"\u001b[39m, env\u001b[39m.\u001b[39maction_space, env\u001b[39m.\u001b[39;49mactionspace\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lena-/Downloads/fc_dqn.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m#1.5 What properties can we expect from the returned observation?\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lena-/Downloads/fc_dqn.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mObservation Space\u001b[39m\u001b[39m\"\u001b[39m, env\u001b[39m.\u001b[39mobservation_space, env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape)\n",
            "File \u001b[1;32mc:\\Users\\lena-\\miniconda3\\envs\\scipy\\lib\\site-packages\\gymnasium\\core.py:282\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39melif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    281\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
            "File \u001b[1;32mc:\\Users\\lena-\\miniconda3\\envs\\scipy\\lib\\site-packages\\gymnasium\\core.py:282\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39melif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    281\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'AtariEnv' object has no attribute 'actionspace'"
          ]
        }
      ],
      "source": [
        "#Task 1: Basic usage\n",
        "\n",
        "#1.1 Create environment\n",
        "env = gym.make('ALE/Breakout-v5')\n",
        "observation = env.reset() #\n",
        "\n",
        "#1.2 How to create the action-state-transition loop?\n",
        "ERP  = []\n",
        "\n",
        "for _ in range(1000):\n",
        "\n",
        "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
        "    next_observation, reward, terminated, truncated, info = env.step(action) #terminated = is goal/trap state?\n",
        "                                                                        #truncated = max step we allow size reached?\n",
        "                                                                        #info = only info for human, not agent\n",
        "    \n",
        "    ERP.append((observation, action, reward, next_observation))\n",
        "\n",
        "\n",
        "#1.3 How can we determine a terminal state?\n",
        "if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "\n",
        "#1.4 Which actions will be accepted?\n",
        "print(\"Action Space\", env.action_space, env.actionspace.shape)\n",
        "\n",
        "#1.5 What properties can we expect from the returned observation?\n",
        "print(\"Observation Space\", env.observation_space, env.observation_space.shape)\n",
        "\"\"\"\n",
        "0 min value\n",
        "255 max value\n",
        "(210,160,3) shape of the Atari Screen \n",
        "unit8\n",
        "\"\"\"\n",
        "\n",
        "#1.6 How can we vectorize it (run multiple in parallel?)\n",
        "#vector()\n",
        "envs = gym.make('ALE/Breakout-v5', num_envs = 3) #specifiy how man you want and batch\n",
        "\n",
        "#1.7 Why is vectorization important?\n",
        "\"\"\"\n",
        "Take 500 states at once of diff envs and batch_size =500 → faster\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBwQS328vi2r"
      },
      "outputs": [],
      "source": [
        "#Task2: Create a DQN\n",
        "import tensorflow as tf\n",
        "import keras \n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "#1.1 Implement a simple DQN\n",
        "def get_breakout_dqn():\n",
        "\n",
        "  #input  = tf.float32(input)/255\n",
        "  input = tf.keras.Input(shape =(210,160,3))\n",
        "  x = tf.keras.layers.Conv2D(5,7, activation = \"relu\")(input) #input (210,160,3), output = action_space.shape\n",
        "\n",
        "  for i in range(5):\n",
        "    x = tf.keras.layers.Conv2D(5,7, activation = \"relu\")(x) \n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  x = tf.keras.layers.Dense(4, activation=tf.nn.softmax)(x)\n",
        "\n",
        "  dqn_model = tf.keras.Model(inputs = input, outputs = x , name =\"dqn\")\n",
        "\n",
        "  return dqn_model\n",
        "  \n",
        "#1.2 Get the Q-value prediction on some dummy inputs\n",
        "\n",
        "dqn = get_breakout_dqn()\n",
        "\n",
        "sample_input = np.expand_dims(env.observation_space.sample(), 0)\n",
        "dqn(sample_input) #output shape = [1,4]\n",
        "\n",
        "#1.3 Write a function to sample an action for a given state, based on an epsilon-greedy policy \n",
        "def sample_trajectory(dqn, state, epsilon=0.2):\n",
        "  terminated = False\n",
        "  trajectory  = []\n",
        "\n",
        "  state, _ = env.reset()\n",
        "\n",
        "  while not terminated:\n",
        "      \n",
        "    q_vals = dqn(tf.expand_dims(state,0)).numpy()\n",
        "    print(q_vals)\n",
        "\n",
        "    #epsilon greedy or Bellman curve with softmax\n",
        "    if random.random() < epsilon:\n",
        "      action = env.action_space.sample()\n",
        "    else: action =np.argmax(q_vals)  #remove bacth dim\n",
        "\n",
        "    next_state, reward, terminated, _ , _ = env.step(action)\n",
        "    trajectory.append((state, action, reward, next_state, terminated))\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "  return trajectory\n",
        "\n",
        "dqn = get_breakout_dqn()\n",
        "sample_trajectory(dqn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKX7k7kJw78E"
      },
      "outputs": [],
      "source": [
        "#Task3 Updating the network\n",
        "\n",
        "#3.1 Compute the Q-targets based on a given rewards r [batch_size], discount factor gamma and the predictions of Q(s',a) [batch_size, actions]\n",
        "\n",
        "gamma = 0.9\n",
        "\n",
        "q_targets  = reward + gamma * dqn(tf.expand_dims(next_state,0)).numpy() #batch dim removen?\n",
        "\n",
        "#3.2 Expand the task above based on a binary variable t, which encodes whether the state s' is a terminal state (1. if terminal, else 0.)\n",
        "if terminal:\n",
        "    q_targets  = reward\n",
        "\n",
        "#3.2 Given a (s, a, r, s', t) tuple, the target network Q_target, and the DQN Q_dqn (and the optimizer dqn_optimizer), write a function implementing a Gradient Descent Update on Q_dqn\n",
        "\n",
        "def gradient_descent_update(sample, targt_net, dqn, gamma =0.9):\n",
        "\n",
        "    optimizer = tf.optimizers.Adam()\n",
        "\n",
        "    state, action, reward, next_state, terminated = sample    \n",
        "\n",
        "    with tf.GradientTape() as tape: \n",
        "        q_values = dqn(tf.expand_dims(next_state,0))\n",
        "        #q_values = tf.reduce_sum(q_values  *tf.)\n",
        "        q_targets = reward +  gamma * targt_net(state, action) \n",
        "\n",
        "        loss = tf.reduce_mean(tf.square(q_targets - q_values))\n",
        "\n",
        "    gradients = tape.gradient(loss, dqn.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, dqn.trainable_variables))\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag1B81px0wIL"
      },
      "outputs": [],
      "source": [
        "#Task4 Polyak Averaging for the target network\n",
        "\n",
        "#4.1 Implement a function with target network Q_target, the DQN Q_dqn, and a polyak-averaging factor tau, which computes and applies a polyak-averaging step on Q_target\n",
        "\n",
        "def apply_polyak_avg (target_net, dqn, tau):\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2_sjf0c1UhT"
      },
      "outputs": [],
      "source": [
        "#Task5 Implementing the Experience Replay Buffer via TF datasets\n",
        "\n",
        "#5.1 Write a function, which samples batch_size many trajectories (list of [s,a,r,s',t] tuples) from a verctorized Breakout game\n",
        "\n",
        "#5.2 Pass each trajectory into a tensorflow dataset\n",
        "\n",
        "#5.3 Provide a function, which implements appropriate preprocessing on the dataset\n",
        "\n",
        "#5.4 Provide a function, given a list of trajectory-datasets samples steps from them (i.e. each step from any trajectory should be equally likely to be drawn!)\n",
        "\n",
        "#5.5 Provide a function, which prepates such a combined-trajectory dataset (which constitutes an ERP!) for usage with DL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPZRfUjG2rPX"
      },
      "outputs": [],
      "source": [
        "#Task6 (HW): Combine the above into a full DQN implementation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
